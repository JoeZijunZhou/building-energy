{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECO's Python M&V model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "from matplotlib import style\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from sklearn import svm, cross_validation, linear_model, preprocessing, ensemble\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "from matplotlib.pyplot import cm \n",
    "from pandas.tseries.offsets import MonthEnd\n",
    "from scipy import stats\n",
    "\n",
    "%matplotlib inline\n",
    "style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class DataGatherer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data(f):\n",
    "    dwnld_data = pd.read_csv(f, index_col=0)\n",
    "    dwnld_data.index = pd.to_datetime(dwnld_data.index)\n",
    "    #print dwnld_data.columns\n",
    "    return dwnld_data\n",
    "\n",
    "def get_data(f):\n",
    "# this in the future will handle data from PI directly instead of data from .csv\n",
    "    return load_data(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class PreProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### Add  Useful Variables and Remove Unused Variables\n",
    "\n",
    "def add_variables(data): # feature extraction function\n",
    "# select columns needed\n",
    "    colmn=[u'chw.bph', u'elec.bph', u'steam.bph', u'oat']#,u'steam.gallons']\n",
    "    data=data.loc[:,colmn]\n",
    "# rename columns\n",
    "    data.columns=['chw','elec','steam','oat']#, \"condensate\"]\n",
    "#add time-dependent variables\n",
    "    data[\"YEAR\"]=data.index.year\n",
    "    data[\"MONTH\"]=data.index.month\n",
    "    data[\"TOD\"]=data.index.hour\n",
    "    data[\"DOW\"]=data.index.weekday\n",
    "    data[\"WEEK\"]=data.index.week\n",
    "    data[\"DOY\"]=data.index.dayofyear\n",
    "\n",
    "# force numeric type for all variables\n",
    "    for col in data.columns:\n",
    "        data[col]=pd.to_numeric(data[col], errors='coerce')\n",
    "        \n",
    "# add heating and cooling degree hours\n",
    "    hdh_point=65\n",
    "    cdh_point=65\n",
    "    data['hdh']=data['oat']\n",
    "    data.loc[data.loc[:,'hdh']>hdh_point,'hdh']=0 \n",
    "    data.loc[data.loc[:,'hdh']<=hdh_point,'hdh']=hdh_point-data.loc[data.loc[:,'hdh']<=hdh_point,'hdh']\n",
    "    data.loc[:,'cdh']=data['oat']\n",
    "    data.loc[data.loc[:,'cdh']<cdh_point,'cdh']=0\n",
    "    data.loc[data.loc[:,'cdh']>=cdh_point,'cdh']=data.loc[data.loc[:,'cdh']>=cdh_point,'cdh']-cdh_point\n",
    "    return data\n",
    "\n",
    "def clean_data(data):\n",
    "# remove outliers: force values withinn band: < mean + 5sd ro remove outliers\n",
    "    n=4\n",
    "    cond_chw_min=0\n",
    "    cond_chw_max=data.chw.mean()+(data.chw.std()*n)\n",
    "    cond_elec_min=0\n",
    "    cond_elec_max=data.elec.mean()+(data.elec.std()*n)\n",
    "    cond_steam_min=0\n",
    "    cond_steam_max=data.steam.mean()+(data.steam.std()*n)\n",
    "    cond_oat_min=data.oat.mean()-(data.oat.std()*n)\n",
    "    cond_oat_max=data.oat.mean()+(data.oat.std()*n)\n",
    "# fill missing variables or cut outliers with mean data\n",
    "    data.loc[((data.chw) < cond_chw_min) | ((data.chw)> cond_chw_max ) , \"chw\"] =data.loc[:,'chw'].mean()\n",
    "    data.loc[((data.elec)< cond_elec_min ) |((data.elec)> cond_elec_max ),\"elec\"]=data.loc[:,'elec'].mean()\n",
    "    data.loc[((data.steam)< cond_steam_min ) | ((data.steam)> cond_steam_max ), \"steam\"]=data.loc[:,'steam'].mean()\n",
    "    data.loc[((data.oat)< cond_oat_min ) | ((data.oat)> cond_oat_max ) ]=data.oat.mean()\n",
    "    data=data.dropna()\n",
    "\n",
    "    data=special_case(data)\n",
    "    return data\n",
    "\n",
    "# bad pracetice do not do that\n",
    "def special_case (data):\n",
    "    for i in range (3):\n",
    "        col=\"elec\"\n",
    "        data.loc[data[col]==data[col].max(),col]=data.loc[:,col].mean()\n",
    "        col=\"chw\"\n",
    "        data.loc[data[col]==data[col].max(),col]=data.loc[:,col].mean()    \n",
    "    return data\n",
    "\n",
    "def remove_negative(dataNoBound):\n",
    "    dataNoBound[dataNoBound < 0]=0\n",
    "    return dataNoBound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class ModelSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_model (model_data, tar, var, algorithm, mod_type, train_start, train_end, val_start, val_end):\n",
    "    ## Model features \n",
    "\n",
    "# 1) Simple model with flat internal load\n",
    "    if mod_type==1:\n",
    "        model_col=var.remove['hdh','cdh']\n",
    "# 2) Model with Internal Gain profile by time, week and month\n",
    "    elif mod_type==2:\n",
    "        model_col=var\n",
    "        add_var=pd.get_dummies(model_data[\"TOD\"],prefix=\"TOD_\") # turn time of day into dummy variables\n",
    "        model_data=model_data.join(add_var) # add all the columns to the model data\n",
    "        model_col=var+add_var.columns.tolist() # full list of variable for regression\n",
    "\n",
    "        add_var=pd.get_dummies(model_data[\"DOW\"],prefix=\"DOW_\") # turn day of week into dummy variables\n",
    "        model_data=model_data.join(add_var) # add all the columns to the model data\n",
    "        model_col=model_col+add_var.columns.tolist() # full list of variable for regression\n",
    "\n",
    "        add_var=pd.get_dummies(model_data[\"MONTH\"],prefix=\"MONTH_\") # turn month into dummy variables\n",
    "        model_data=model_data.join(add_var) # add all the columns to the model data\n",
    "        model_col=model_col+add_var.columns.tolist() # full list of variable for regression\n",
    "\n",
    "# Select Training Set\n",
    "    data_train=model_data.loc[train_start:train_end, model_col]#.dropna() # slice training period\n",
    "    #if val_start & val_end\n",
    "    data_train=data_train.drop(data_train[val_start:val_end].index) #remove validation intervals\n",
    "    target_train=model_data.loc[train_start:train_end,tar]#.dropna()\n",
    "    target_train=target_train.drop(target_train[val_start:val_end].index) #remove validation intervals\n",
    "\n",
    "\n",
    "# Train a simple linear model\n",
    "    if algorithm==1:\n",
    "        clf = linear_model.LinearRegression()    \n",
    "    elif algorithm==2:\n",
    "        clf = ensemble.RandomForestRegressor()\n",
    "        \n",
    "    model=clf.fit(data_train,target_train)\n",
    "\n",
    "# Save the predicted target\n",
    "    target_modeled_train=model.predict(data_train)\n",
    "# Set negative values (energy) to zero\n",
    "    target_modeled_train=remove_negative(target_modeled_train)\n",
    "\n",
    "# save actual target and predicted target side by side\n",
    "    compare=pd.DataFrame(target_train)\n",
    "    compare.columns=[\"target_actual\"]\n",
    "    compare[\"target_predicted\"]=target_modeled_train    \n",
    "# Save the score\n",
    "    score=model.score(data_train,target_train) #replace this with functions\n",
    "\n",
    "# Print model coefficients (to see what are the weights)\n",
    "#    print \"Model variables const + %s\" %model_col \n",
    "#    print \"Model coeff %s + %s\"   % (model.intercept_, model.coef_)\n",
    "\n",
    "    return {\"model\":model,\"data_train\":data_train,\"target_train\":target_train,\"model_data\":model_data,\n",
    "            \"model_col\":model_col,\"score\":score, \"target_modeled_train\":target_modeled_train, \"compare\":compare}\n",
    "\n",
    "def predict_model (model_data, tar, var, model, val_start, val_end):\n",
    "    # Select Validation set \n",
    "    model_col=var\n",
    "# select columns and lines\n",
    "    data_val=model_data.loc[val_start:val_end,model_col]#.dropna()\n",
    "    target_val=model_data.loc[val_start:val_end,tar]#.dropna()\n",
    "\n",
    "# Save the predicted target\n",
    "    target_modeled_val=model.predict(data_val)\n",
    "# Set negative values (energy) to zero\n",
    "    target_modeled_val=remove_negative(target_modeled_val)\n",
    "\n",
    "# save actual target and predicted target side by side\n",
    "    compare=pd.DataFrame(target_val)\n",
    "    compare.columns=[\"target_actual\"]\n",
    "    compare[\"target_predicted\"]=target_modeled_val    \n",
    "# Save the score\n",
    "    score=model.score(data_val,target_val)\n",
    "\n",
    "    return {\"score\":score, \"target_modeled_val\":target_modeled_val, \"compare\":compare}\n",
    "\n",
    "def calc_scores(compare,p):\n",
    "    scores={}\n",
    "    \n",
    "    n=compare.count()[1]\n",
    "    R2=r2_score(compare[\"target_actual\"], compare[\"target_predicted\"]) # this can be negative\n",
    "    RMSE=(mean_squared_error(compare[\"target_actual\"], compare[\"target_predicted\"])*n/(n-p))**(0.5)\n",
    "    CV_RMSE=RMSE*100/compare[\"target_actual\"].mean()\n",
    "    NMBE =compare[\"target_actual\"].sub(compare[\"target_predicted\"]).sum()/(compare[\"target_predicted\"].mean())/(n-p)*100\n",
    "    scores[\"Adj_R2\"]= 1-(1-R2)*(n-1)/(n-p-1)\n",
    "    scores[\"RMSE\"]=RMSE\n",
    "    scores[\"CV_RMSE\"]=CV_RMSE\n",
    "    scores[\"NMBE\"]=NMBE\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class SavingCalculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_savings(model_data, tar, var, model, val_start, val_end):\n",
    "    return predict_model(model_data, tar, var, model, val_start, val_end)\n",
    "\n",
    "#def savings_table_byMo (data_, tar, cost  ):\n",
    "#    data_[tar]=(data_[\"target_predicted\"]-data_[\"target_actual\"])*cost[tar]\n",
    "#    #data_[tar+\"sav_perc\"]=data_[tar]/(data_[\"target_predicted\"]+0.01)*100 for savings %\n",
    "#    time_res=\"M\"\n",
    "#    data_table=pd.DataFrame(data_.groupby(pd.TimeGrouper(time_res)).sum()[tar])\n",
    "#    return data_table.to_dict()\n",
    "\n",
    "def savings_table_byMo (data_, tar, cost  ):\n",
    "# for cost savings\n",
    "    time_res=\"M\"\n",
    "    data_cost_sav={}\n",
    "    data_perc_sav={}\n",
    "    data_=pd.DataFrame(data_.groupby(pd.TimeGrouper(time_res)).sum())\n",
    "# cost savings \n",
    "    data_[tar]=(data_[\"target_predicted\"]-data_[\"target_actual\"])*cost[tar]\n",
    "# % savings\n",
    "    data_[tar+\"sav_perc\"]=(data_[\"target_predicted\"]-data_[\"target_actual\"])/(data_[\"target_predicted\"]+0.01)*100\n",
    "    #data_cost_sav=pd.DataFrame(data_.groupby(pd.TimeGrouper(time_res)).sum()).to_dict()\n",
    "    #data_perc_sav=pd.DataFrame(data_.groupby(pd.TimeGrouper(time_res)).mean()[tar+\"sav_perc\"])#.to_dict()\n",
    "    data_cost_sav[tar] = data_[tar].to_dict()\n",
    "    data_perc_sav[tar] =data_[tar+\"sav_perc\"].to_dict()\n",
    "    return {\"data_cost_sav\":data_cost_sav, \"data_perc_sav\":data_perc_sav}\n",
    "\n",
    "#def calc_uncert(compare_train, confidence, score_tot, tar, absol):\n",
    "#    alpha=(100-confidence)/100. #0.05 = 95% confidence ; t-statistic 2-tail\n",
    "#    n=(len(compare_train))\n",
    "#    t_stat=stats.t.ppf(1-alpha/2, n)\n",
    "#    RMSE_val=score_tot[tar][\"score_train\"][\"RMSE\"]\n",
    "#    smpl_siz=math.sqrt((1+1/(len(compare_train)*1.0)))\n",
    "#    \n",
    "#    if absol==True:\n",
    "#        return t_stat*RMSE_val*smpl_siz #absolute error\n",
    "#    else:\n",
    "#        return t_stat*RMSE_val*smpl_siz/compare_sav[\"target_actual\"].mean()*100\n",
    "\n",
    "def calc_uncert(compare_train, compare_sav, confidence, score_tot, tar, absol):\n",
    "    alpha=(100-confidence)/100. #alpha=0.05 for 95% confidence ; t-statistic 2-tail\n",
    "    n=(len(compare_train)) # n samples in training\n",
    "    m=(len(compare_val)) # m samples in validation\n",
    "    \n",
    "# adjust n due to autocorrelation of residuals in training (see ASHRAE guideline 14 - annex B2) \n",
    "    res=compare_train[\"target_predicted\"]-compare_train[\"target_actual\"]\n",
    "    rho=res.autocorr()\n",
    "    n_p= n*(1-rho)/(1+rho) # n' in annex B2\n",
    "    \n",
    "    smpl_siz=(n/n_p*(1+(2/n_p))*1/m)**0.5\n",
    "    #smpl_siz=math.sqrt((n+2)/(n*m*1.0))\n",
    "    sav_perc=(res/compare_train[\"target_predicted\"]).sum()/100\n",
    "    \n",
    "    t_stat=stats.t.ppf(1-alpha/2, n)\n",
    "    CV_RMSE_val=score_tot[tar][\"score_train\"][\"CV_RMSE\"]\n",
    "    #smpl_siz=math.sqrt((n+2)/(n*m*1.0))\n",
    "    \n",
    "    print n, m, t_stat, CV_RMSE_val, smpl_siz\n",
    "    if absol==True:\n",
    "        return 1.26*t_stat*CV_RMSE_val*smpl_siz/sav_perc #+- % error\n",
    "    else:\n",
    "        return 1.26*t_stat*CV_RMSE_val*smpl_siz*(compare_sav[\"target_actual\"].sum())    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class Plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_energy_profile_byMo (date_start, date_end, data, tar):\n",
    "    # daily energy profile by Month (each line is an avergage month)\n",
    "    data_graph=data[date_start: date_end]\n",
    "    data_graph[\"YR-MO\"]=data_graph.YEAR.astype(int).astype(str).str.cat(data_graph.MONTH.astype(int).astype(str), sep='-')    \n",
    "    temp=data_graph.groupby([\"TOD\",\"YR-MO\"]).mean()[tar].unstack()\n",
    "    n=len(temp.columns)\n",
    "    color=cm.rainbow(np.linspace(0,1,n))\n",
    "    temp.plot(figsize=(15,5),label=tar, color=color, title=tar,ylim=[0,temp.max().max()*1.1])\n",
    "    plt.show()\n",
    "\n",
    "def plot_WD_WE (data, tar ):\n",
    "    WD=data[\"DOW\"]<5\n",
    "    WE=data[\"DOW\"]>=5\n",
    "    time_res=\"d\"\n",
    "    WD_tmp=data[WD].groupby(pd.TimeGrouper(time_res)).mean().loc[:,tar]\n",
    "    WD_tmp.plot(figsize=(15,5), style='o', title=tar,ylim=[0,WD_tmp.max()*1.1]) #WD\n",
    "    WE_tmp=data[WE].groupby(pd.TimeGrouper(time_res)).mean().loc[:,tar]\n",
    "    WE_tmp.plot(figsize=(15,5), style='o', title=tar, ylim=[0,WE_tmp.max()*1.1]) #WE\n",
    "    plt.show()\n",
    "        \n",
    "    data_plot=pd.DataFrame()\n",
    "    data_plot[\"WD\"]=data[WD].groupby(pd.TimeGrouper(time_res)).mean().loc[:,tar]\n",
    "    data_plot[\"WE\"]=data[WE].groupby(pd.TimeGrouper(time_res)).mean().loc[:,tar]\n",
    "    data_plot.plot( kind=\"box\",figsize=(15,5), title=tar, ylim=[0,data_plot.max().max()*1.1])\n",
    "    plt.show()\n",
    "    \n",
    "def plot_scatter (data, tar, x_var, perPre, perPost):\n",
    "    dataPre=data[\"PrePost\"]==perPre\n",
    "    dataPost=data[\"PrePost\"]==perPost\n",
    "    ax=data[dataPre].plot(kind=\"scatter\", x=x_var, y=tar,figsize=(15,5), title=tar)\n",
    "    data[dataPost].plot(kind=\"scatter\", x=x_var, y=tar,figsize=(15,5), color=\"r\", ax=ax)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_compare (compare_data,plot_start,plot_end):\n",
    "    compare_data.loc[plot_start:plot_end,:].plot(figsize=(15,5),title=tar)#.set_title(\"month = %d\" %month)  \n",
    "    plt.show() \n",
    "\n",
    "def plot_PrePost_byMo (data, tar):\n",
    "    last_mo=data[data[\"PrePost\"]==1].index.max().month\n",
    "    temp=data.groupby([\"MONTH\",\"YEAR\"])[tar].mean().unstack()\n",
    "    temp[(temp.index<=last_mo)].plot(figsize=(15,5), kind=\"bar\",title=tar)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_ModPost_byMo (compare_sav, tar):\n",
    "    cols=[\"target_predicted\", \"target_actual\"]\n",
    "    compare_sav = compare_sav.ix[:, cols]\n",
    "    compare_sav.groupby(compare_sav.index.month).mean().plot(figsize=(15,5), kind=\"bar\",title=tar)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_PrePostSav_byMo(data, tar):\n",
    "    last_mo=data[data[\"PrePost\"]==1].index.max().month\n",
    "    temp=data.groupby([\"MONTH\",\"YEAR\"])[tar].mean().unstack()\n",
    "    (temp[(temp.index<=last_mo)].diff(axis=1)*(-1)).plot(figsize=(15,5), kind=\"bar\",title=tar)\n",
    "    plt.show()\n",
    "\n",
    "def plot_ModPostSav_byMo(compare_sav, tar):\n",
    "    cols=[\"target_predicted\", \"target_actual\"]\n",
    "    compare_sav = compare_sav.ix[:, cols]\n",
    "    (compare_sav.groupby(compare_sav.index.month).mean().diff(axis=1)*(-1)).plot(figsize=(15,5), kind=\"bar\",title=tar)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_daily_profile (compare_data):\n",
    "# add select and group by variables\n",
    "    compare_data[\"MONTH\"]=compare_data.index.month\n",
    "    compare_data[\"TOD\"]=compare_data.index.hour\n",
    "    mi=compare_data[\"MONTH\"].min()\n",
    "    ma=compare_data[\"MONTH\"].max()\n",
    "\n",
    "    for month in range (mi,ma+1):\n",
    "        cond=compare_data[\"MONTH\"]==month\n",
    "        col=['target_actual', 'target_predicted', 'TOD']\n",
    "        compare_red=compare_data.loc[cond,col]\n",
    "        compare_grouped=compare_red.groupby(\"TOD\").mean()\n",
    "        compare_grouped.plot(figsize=(18,5)).set_title(\"month = %d\" %month)    \n",
    "#    plt.close() \n",
    "\n",
    "def plot_savings(compare,time_res):\n",
    "    compare_val.diff(periods=1, axis=1).iloc[:,1].groupby(\n",
    "        pd.TimeGrouper(time_res)).mean().interpolate(method='time').plot(kind=\"bar\", figsize=(10,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "TVZfUHJvY2VkdXJlLnBuZw==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"MV_Procedure.png\")\n",
    "# diagram of the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# going to be configuration file with all variables\n",
    "\n",
    "cost_ele =0.0766/3412.14 #\n",
    "cost_steam =0.10025/9126 # $/gal * gal/BTU ***note: use hourly data here\n",
    "cost_chw =0.1/12000  #\n",
    "\n",
    "cost={\"elec\":cost_ele,\n",
    "     \"steam\":cost_steam,\n",
    "     \"chw\":cost_chw}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File data\\Ghausi_M&V Data.csv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-05f6161ffa03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[1;31m# LOAD DATA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mdwnld_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFOLDER\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mFILE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[1;31m# CLEAN DATA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-b6c2096fd4db>\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[1;31m# this in the future will handle data from PI directly instead of data from .csv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-b6c2096fd4db>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdwnld_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdwnld_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdwnld_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[1;31m#print dwnld_data.columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdwnld_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    644\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    645\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    921\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 923\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    924\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1388\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1390\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas\\parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas\\parser.c:4184)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas\\parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas\\parser.c:8449)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: File data\\Ghausi_M&V Data.csv does not exist"
     ]
    }
   ],
   "source": [
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "   \n",
    "    # file source variables\n",
    "    FOLDER='data'\n",
    "    FILE= 'Ghausi_M&V Data.csv'\n",
    "    #FILE= 'PES_M&V Data.csv'\n",
    "    \n",
    "# LOAD DATA\n",
    "    dwnld_data=get_data(os.path.join(FOLDER,FILE))\n",
    "\n",
    "# CLEAN DATA\n",
    "    data=add_variables(dwnld_data)   \n",
    "    data=clean_data(data)\n",
    "    #print data\n",
    "\n",
    "## TASKS\n",
    "    explor=False #plots exploratory graphs\n",
    "    search=True # iterates to get best model\n",
    "    \n",
    "# define pre-post    \n",
    "    pre_start =pd.to_datetime('4,1,2016',  format='%m,%d,%Y', errors ='coerce', dayfirst=False)\n",
    "    pre_end = pd.to_datetime('11,1,2016', format='%m,%d,%Y', errors ='coerce', dayfirst=False)\n",
    "    post_start =pd.to_datetime('11,1,2016', format='%m,%d,%Y', errors ='coerce', dayfirst=False)\n",
    "    post_end = pd.to_datetime('11,1,2017', format='%m,%d,%Y', errors ='coerce', dayfirst=False)\n",
    "    data[\"PrePost\"]=np.nan\n",
    "    data.loc[pre_start:pre_end,\"PrePost\"]=0\n",
    "    data.loc[post_start:post_end,\"PrePost\"]=1\n",
    "\n",
    "# select energy type variables\n",
    "    energy_type=[\"chw\",\"elec\", \"steam\"]\n",
    "    #energy_type=[\"chw\"]\n",
    "# select other regressors\n",
    "    var = ['hdh','cdh']\n",
    "# select regression algorithm    \n",
    "    algorithm =2\n",
    "    mod_type =2\n",
    "\n",
    "### PLOTS FOR EACH ENERGY TYPE\n",
    "# PLOT GROUP 1: DATA EXPLORATORY GRAPHS\n",
    "    if explor==True:\n",
    "        for tar in energy_type:\n",
    "# PLOT G1-1: profile of energy by hour of day, a line per month       \n",
    "            #plot_energy_profile_byMo(\"2012\", \"2013\", data, tar)\n",
    "        \n",
    "# PLOT G1-2: WD/WE profiles in time \n",
    "# note: add holidays !!!! \n",
    "            #plot_WD_WE (data, tar) \n",
    "\n",
    "# PLOT G1-3: scatter plot\n",
    "            x_var=\"oat\"\n",
    "            perPre=0\n",
    "            perPost=1\n",
    "            plot_scatter (data, tar, x_var, perPre, perPost)\n",
    "        \n",
    "    print \"data time range:  %s - %s \\n\" % (data.index.min(), data.index.max())\n",
    "\n",
    "### FOR LOOP 1: cycle through energy types    \n",
    "    score_tot={}\n",
    "    saving_cost={}\n",
    "    saving_perc={}\n",
    "    uncert_tot={}\n",
    "    \n",
    "    \n",
    "    for tar in energy_type: # as an alternative to define it explicitely use k-fold cross validation with sci-kit learn...\n",
    "\n",
    "# Training Period - Use the full period, then the validation will be dropped later\n",
    "        train_start =pre_start\n",
    "        train_end = pre_end\n",
    "# Validation Period; va\n",
    "        val_start =pd.to_datetime('4,1,2015',  format='%m,%d,%Y', errors ='coerce', dayfirst=False)\n",
    "        val_end = pd.to_datetime('4,7,2015',  format='%m,%d,%Y', errors ='coerce', dayfirst=False)\n",
    "\n",
    "# period to calculate savings over\n",
    "        pred_start = post_start\n",
    "        pred_end = post_end\n",
    "        \n",
    "        data.loc[train_start:train_end,\"Period\"]=1\n",
    "        data.loc[val_start:val_end,\"Period\"]=2\n",
    "        data.loc[pred_start:pred_end,\"Period\"]=3\n",
    "               \n",
    "        print \"Target Var is        %s\" % (tar)\n",
    "        print \"Training time range:   %s - %s\" % (train_start, train_end)\n",
    "        print \"Validation time range: %s - %s\" % (val_start, val_end)\n",
    "        \n",
    "# TRAIN MODEL        \n",
    "        ret_obj=train_model (data, tar, var, algorithm, mod_type, train_start, train_end, val_start, val_end)\n",
    "    \n",
    "# save results\n",
    "        curr_model=ret_obj[\"model\"]\n",
    "        model_col=ret_obj[\"model_col\"]\n",
    "        model_data=ret_obj[\"model_data\"]\n",
    "        score_train=ret_obj[\"score\"]\n",
    "        target_modeled_train=ret_obj[\"target_modeled_train\"]\n",
    "        compare_train=ret_obj[\"compare\"]\n",
    "        #temp=ret_obj[\"data_train\"]\n",
    "\n",
    "#save scores\n",
    "        score_tot[tar]={}\n",
    "        score_tot[tar][\"score_train\"]=calc_scores(compare_train,len(model_col)) \n",
    "        print \"score train = \"\n",
    "        print score_tot[tar][\"score_train\"]\n",
    "        \n",
    "# PREDICT AND COMPARE\n",
    "        ret_obj2=predict_model(model_data, tar, model_col, curr_model, val_start, val_end)   \n",
    "\n",
    "    # save results\n",
    "        score_val=ret_obj2[\"score\"]\n",
    "        target_modeled_val=ret_obj2[\"target_modeled_val\"]\n",
    "        compare_val=ret_obj2[\"compare\"]\n",
    "    \n",
    "        score_tot[tar][\"score_val\"]=calc_scores(compare_val,len(model_col)) \n",
    "        print \"score val = \"\n",
    "        print score_tot[tar][\"score_val\"]\n",
    "        print \"\\n\"\n",
    "        score_table=pd.Panel(score_tot).transpose(1,0,2).to_frame() #build score table\n",
    "        \n",
    "# CALCULATE SAVINGS\n",
    "        ret_obj3=calc_savings(model_data, tar, model_col, curr_model, pred_start, pred_end) \n",
    "        compare_sav=ret_obj3[\"compare\"]\n",
    "        \n",
    "        ret_obj4=savings_table_byMo(compare_sav, tar, cost)\n",
    "        saving_cost.update(ret_obj4[\"data_cost_sav\"])\n",
    "        saving_perc.update(ret_obj4[\"data_perc_sav\"])\n",
    "        \n",
    "# UNCERTAINTY        \n",
    "        #uncert_tot.update(calc_uncert)\n",
    "        print calc_uncert(compare_train, compare_sav, 95, score_tot, tar, absol=True)\n",
    "# PLOT MODEL\n",
    "        plot_compare(compare_train,compare_train.index.min(),compare_train.index.max())\n",
    "        plot_compare(compare_val,compare_val.index.min(),compare_val.index.max())\n",
    "        plot_compare(compare_sav,compare_sav.index.min(), compare_sav.index.max())\n",
    "# PLOT SAVINGS        \n",
    "        plot_PrePost_byMo(data, tar)\n",
    "        plot_ModPost_byMo(compare_sav, tar)\n",
    "        plot_PrePostSav_byMo(data, tar)\n",
    "        plot_ModPostSav_byMo(compare_sav, tar)\n",
    "\n",
    "    saving_cost=pd.DataFrame(saving_cost).T\n",
    "    saving_perc=pd.DataFrame(saving_perc).T\n",
    "\n",
    "    print score_table\n",
    "    print saving_cost\n",
    "    print saving_perc\n",
    "    print saving_cost.sum()[2:].sum()\n",
    "    print saving_perc.sum()[2:].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "saving_cost\n",
    "saving_cost.transpose().to_csv(\"saving_cost2.csv\", index=True)\n",
    "#saving_cost.to_csv(\"saving_cost.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def year_savings():\n",
    "# project for 1 year\n",
    "    time_res=\"M\"\n",
    "    fuel=[\"chw\",\"elec\",\"steam\"]\n",
    "    now=pd.to_datetime('7,1,2016', format='%m,%d,%Y', errors ='coerce', dayfirst=False) #data.index.max()\n",
    "    beginning=now-pd.Timedelta(\"366 days\")\n",
    "    end=pd.to_datetime('7,1,2016', format='%m,%d,%Y', errors ='coerce', dayfirst=False)\n",
    "    data_annual=data[beginning:end].groupby(pd.TimeGrouper(time_res)).sum()\n",
    "    #print data[beginning:end]\n",
    "\n",
    "    tot={}\n",
    "    for targ in fuel:\n",
    "        tot[targ]=(data_annual[targ]*saving_perc.T.loc[:,targ].mean()/100*cost[targ]).sum() + saving_cost.T.loc[:,targ].sum()\n",
    "    tot=pd.Series(tot)\n",
    "    print tot#.sum()\n",
    "    return tot.sum()\n",
    "year_savings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#savings $ per Mo\n",
    "saving_cost.T[2:]#.T.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# savings avg %\n",
    "saving_perc.T[2:]#.T.mean().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# to do\n",
    "\n",
    "# DEEPER (Ghausi/PES $ story) OK\n",
    "# $ savings in graph/table OK\n",
    "# uncertainty OK\n",
    "# year-projection\n",
    "\n",
    "# compare Ghausi 2010 - ? daily\n",
    "# compare PES 2011 -? daily\n",
    "# disaggregate HVAC-FAN from elec\n",
    "# holidays in model\n",
    "\n",
    "# BROADER (extention to )\n",
    "# weekly profile\n",
    "# KPIs, rolling EUI\n",
    "# classifying profiles (Andrea)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def savings_table_byMo (data_, tar, cost  ):\n",
    "# for cost savings\n",
    "    time_res=\"M\"\n",
    "    data_cost_sav={}\n",
    "    data_perc_sav={}\n",
    "    data_=pd.DataFrame(data_.groupby(pd.TimeGrouper(time_res)).sum())\n",
    "# cost savings \n",
    "    data_[tar]=(data_[\"target_predicted\"]-data_[\"target_actual\"])*cost[tar]\n",
    "# % savings\n",
    "    data_[tar+\"sav_perc\"]=(data_[\"target_predicted\"]-data_[\"target_actual\"])/(data_[\"target_predicted\"]+0.01)*100\n",
    "    #data_cost_sav=pd.DataFrame(data_.groupby(pd.TimeGrouper(time_res)).sum()).to_dict()\n",
    "    #data_perc_sav=pd.DataFrame(data_.groupby(pd.TimeGrouper(time_res)).mean()[tar+\"sav_perc\"])#.to_dict()\n",
    "    data_cost_sav[tar] = data_[tar].to_dict()\n",
    "    data_perc_sav[tar] =data_[tar+\"sav_perc\"].to_dict()\n",
    "    return {\"data_cost_sav\":data_cost_sav, \"data_perc_sav\":data_perc_sav}\n",
    "\n",
    "ret_obj4=savings_table_byMo(compare_sav, tar, cost)\n",
    "saving_cost={}\n",
    "saving_perc={}\n",
    "saving_cost.update(ret_obj4[\"data_cost_sav\"])\n",
    "saving_perc.update(ret_obj4[\"data_perc_sav\"])\n",
    "\n",
    "#print saving_perc\n",
    "#print saving_cost\n",
    "\n",
    "#saving_cost=\n",
    "saving_cost=pd.DataFrame(saving_cost).T\n",
    "saving_perc=pd.DataFrame(saving_perc).T\n",
    "saving_perc\n",
    "#print saving_cost\n",
    "#print saving_perc\n",
    "#print saving_cost.sum()[2:].sum()\n",
    "#print saving_perc.sum()[2:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "saving_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tar=\"steam\"\n",
    "dd=\"hdh\"\n",
    "(data[\"2015\"].groupby(\"MONTH\").sum()[dd]).plot()\n",
    "plt.show()\n",
    "((data[\"2015\"].groupby(\"MONTH\").sum()[tar])).plot()\n",
    "#plt.scatter(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Testing new stuff from here on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## MANUAL DATA CLEANING\n",
    "# run existing code with n=4\n",
    "## chw:  remove hours with 0 use and oat > 65F\n",
    "## steam: remove hours with 0 use and oat < 65F\n",
    "## elec: nothing \n",
    "\n",
    "\n",
    "#a=pd.read_csv('data/Ghausi_M&V Data.csv', index_col=0)\n",
    "#a[[\"chw.bph\",\"oat\"]].plot(figsize=(18,5))\n",
    "#a[[\"chw.bph\",\"oat\"]].plot(kind=\"scatter\",x=\"oat\",y=\"chw.bph\", figsize=(18,5))\n",
    "\n",
    "#data.loc[\"2013\",[\"chw\",\"oat\"]].plot(figsize=(18,5))\n",
    "#data.loc[\"2013\",[\"chw\",\"oat\"]].plot(kind=\"scatter\",x=\"oat\",y=\"chw\", figsize=(18,5))\n",
    "\n",
    "cond=~((data[\"oat\"]>65)&(data[\"chw\"]==0))\n",
    "data_clean=data.loc[cond,:]\n",
    "data_clean.loc[\"2013\",[\"chw\",\"oat\"]].plot(figsize=(18,5))\n",
    "data_clean.loc[\"2013\",[\"chw\",\"oat\"]].plot(kind=\"scatter\",x=\"oat\",y=\"chw\", figsize=(18,5))\n",
    "data_clean.loc[:,[\"chw\",\"oat\"]].to_csv(\"data/clean/Ghausi_M&V_Data_chw_clean.csv\")\n",
    "\n",
    "cond=~((data[\"oat\"]<65)&(data[\"steam\"]==0))\n",
    "data_clean=data.loc[cond,:]\n",
    "data_clean.loc[\"2015\",[\"steam\",\"oat\"]].plot(figsize=(18,5))\n",
    "data_clean.loc[\"2015\",[\"steam\",\"oat\"]].plot(kind=\"scatter\",x=\"oat\",y=\"steam\", figsize=(18,5))\n",
    "data_clean.loc[:,[\"steam\",\"oat\"]].to_csv(\"data/clean/Ghausi_M&V_Data_steam_clean.csv\")\n",
    "\n",
    "cond=((data[\"elec\"]>200000))\n",
    "data_clean=data.loc[cond,:]\n",
    "data_clean.loc[\"04-01-2016\":,[\"elec\",\"oat\"]].plot(figsize=(18,5))\n",
    "data_clean.loc[\"04-01-2016\":,[\"elec\",\"oat\"]].plot(kind=\"scatter\",x=\"oat\",y=\"elec\", figsize=(18,5))\n",
    "data_clean.loc[:,[\"elec\",\"oat\"]].to_csv(\"data/clean/Ghausi_M&V_Data_elec_clean.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cond=~((data[\"oat\"]<65)&(data[\"steam\"]==0))\n",
    "data_clean=data.loc[cond,:]\n",
    "data_clean.loc[:,[\"steam\",\"oat\"]].groupby(pd.TimeGrouper(freq='M')).mean().plot(figsize=(18,5))\n",
    "data_clean.loc[:,[\"steam\",\"oat\"]].groupby(pd.TimeGrouper(freq='M')).mean().plot(kind=\"scatter\",x=\"oat\",y=\"steam\", figsize=(18,5))\n",
    "data_clean.loc[:,[\"steam\",\"oat\"]].to_csv(\"data/clean/Ghausi_M&V_Data_steam_clean.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
